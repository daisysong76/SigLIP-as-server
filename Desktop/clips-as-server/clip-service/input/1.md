Looking at the test code, we're simulating inputs in two ways:

1. Text inputs:
```python
texts = [
    "a dog playing in the park",
    "a cat sleeping on a couch",
    "a sunset over the ocean", 
    "a busy city street at night",
    "a forest with tall trees"
]
```
- Base size: 5 text prompts
- In large batch test: `large_texts = test_data["texts"] * 100` creates 500 text inputs

2. Image inputs:
```python
# Create simple test images (colored squares)
images = []
colors = [(255,0,0), (0,255,0), (0,0,255), (255,255,0), (255,0,255)]
for color in colors:
    img = Image.new('RGB', (224, 224), color)
    images.append(img)
```
- 5 synthetic images
- Each image is 224x224 pixels (standard CLIP input size)
- Simple colored squares (red, green, blue, yellow, magenta)

The embeddings generated have these characteristics:
- Text/Image embedding dimension: 512 (CLIP ViT-B/32 model)
- Data type: float32
- Memory per embedding: 512 * 4 bytes = 2KB
- Total memory for 500 embeddings: ~1MB

Would you like me to:
1. Test with larger input sizes?
2. Use real images instead of synthetic ones?
3. Add more diverse text prompts?
4. Add stress tests with different data sizes?
